\chapter{Теория оценок}
\disclaimer{
    В этой главе представлено краткое введение в раздел математической статистики под названием "Теория оценок". Материал в этой главе нужен только тем, кто хочет получить хорошее качественное представление о теории анализа данных. Остальным рекомендуется ознакомиться только с материалом раздела \ref{sec:chi2}
}

\section{Понятие о точечной оценке}

    Математическая статистика имеет огромное количество разнообразнейших
применений, но с точки зрения экспериментальной физики (и как следствие
студентов, изучающих эту науку) наиболее интересным применением является
оценка параметров закономерностей. Пусть есть некоторое явление природы,
которое можно описать при помощи модели $M(\theta)$. Здесь $\theta$
- это некоторый набор параметров модели, которые могут принимать
различные значения. На этом этапе мы не оговариваем, как именно модель
описывает процесс и что мы можем принимать в качестве параметров.
Положим теперь, что существует некоторый выделенный набор параметров
$\theta_0$, который соответствует некоторому ``истинному'' состоянию
природы. Далее мы будем исходить из того предположения, что при попытке
предпринять некоторые измерения, мы будем получать результаты,
соответствующие нашей модели именно с этим набором параметров.

\textbf{Замечание} Тут важно заметить, что мы также негласно
предполагаем, что природа вообще действует согласно нашей модели, но
этот вопрос мы пока оставим за кадром. В какой-то мере мы вернемся к
нему, в главе 5, когда будем обсуждать теорию проверки гипотез.

Предаставим теперь, что мы провели некоторую серию экспериментов
$X = \{X_0, X_1,...X_N\}$, в которых мы тем или иным способом изучаем
состояние природы (будем дальше называть результаты этих экспериментов
экспериментальной выборкой). Нашей задачей в этой главе будет описание
процедуры, при помощи которой можно на основе выборки сделать вывод об
истинном состоянии природы $\theta_0$. Важно понимать, что в общем
случае, результаты измерений являются случайными величинами, поэтому
полученное нами на основании этих данных состояние природы также будет
случайной величиной в противовес истинному состоянию природы
$\theta_0$, которое вообще говоря, истинно случайной величиной не
является. Полученную величину будем называть \emph{точечной оценкой}
состояния природы $\hat{\theta}$ или просто оценкой. Саму процедуру, в
процессе которой получена оценка, будем называть оцениванием.

\textbf{Пример} Положим, что знания студента в области физики являются
состоянием природы (а точнее данного конкретного студента). Очевидно,
что досконально проверить этот факт не представляется возможным, поэтому
для измерения этой величины мы проводим эксперимент - экзамен. То, что
по результатам экзамена оказывается в ведомости является оценкой не
только с точки зрения деканата, но и с точки зрения математической
статистики.

В дальнейшем будем считать, что состояния природы описываются
действительным числом или набором действительных чисел. Сама по себе
теория этого не требует, но в противном случае довольно сложно
сравнивать состояния между собой (требуется определять понятие близости
в произвольном пространстве). В этом случае наша процедура оценивания:
\begin{equation}
    \hat{\theta} = f(X)
\end{equation} является действительной функцией на пространстве векторов
$X$, состоящих из случайных переменных. Такие функции еще называют
статистиками. Очевидно, что далеко не людая такая функция будет давать
тот результат, которого мы хотим. Поэтому вводятся дополнительные
обязательные свойства оценок.

\section{Свойства точечных оценок}

\subsection{Состоятельность}

Естественное пожелание к оценщику, заключается в том, что качество
оценки должно зависеть от объема выборки, числа измерений $n$
случайных переменных $X$: чем больше выборка, тем качественней оценка
$\hat{\theta}$. Иными словами, мы хотим, чтобы с ростом объема выборки
значение оценки приближалось к истинному значению параметра. При
использовании сходимости по вероятности оценку $\hat{\theta}$
определяют как состоятельную, если при любых $\varepsilon > 0$ и
$\eta > 0$, найдется такое $N$, что вероятность $P \left(
\left| \hat{\theta} - \theta \right| > \varepsilon \right) \textless{} \eta $ при всех $n > N$. Или другими
словами, всегда можно выбрать такое количество измерений, для которого
вроятность отклонения оценки от истинного значения не превышает наперед
заданное число.

\note{
Нужно заметить, что на практике оценки являются
состоятельными только когда при построении оценки не учитывается
систематическая ошибка. В противном случае, может наблюдаться сходимость
по вероятности не к нулю, а к некоторой фиксированной константе.
}

\subsection{Несмещенность}

Рассмотрим набор измерений, каждое из которых состоит из $n$
наблюдений $X$, характеризуемый функцией плотности вероятности
$P(X | \theta) = P(\hat\theta | \theta)$ при фиксированном $n$ и
определим смещение как отклонение среднего по этому набору
$\hat{\theta_n}$ от истинного \begin{equation}
   b = E[\hat{\theta_n}] - \theta
\end{equation} Оценка называется несмещенной, если $b = 0$.

Заметим, что смещение не зависит от измеренных величин, но зависит от
размера образца, формы оценщика и от истинных (в общем случае
неизвестных) свойств ФПВ $f(x)$, включая истинное значение параметра.
Если смещение исчезает в пределе $n \to \infty$, говорят об
асимптотически несмещенной оценке. Заметим, что из состоятельности
оценки не следует несмещенность. Это означает, что даже если
$\hat{\theta}$ сходится к истинной величине $\theta$ в единичном
эксперименте с большим числом измерений, нельзя утверждать, что среднее
$\hat{\theta}$ по бесконечному числу повторений эксперимента с
конечным числом измерений $n$ будет сходится к истинному $\theta$.
Несмещенные оценки пригодны для комбинирования результатов разных
экспериментов. В большинстве практических случаев смещение должно быть
мало по сравнению со статистической ошибкой и им пренебрегают.

\subsection{Несмещённая оценка среднего}

\todo[author = Nozik]{Надо этому столько времени уделять?}

Рассмотрим один конкретный пример смещенных оценок. Формула (\ref{eq:sigma}) $s_{x}^{2}=\frac{1}{n}\sum\Delta x_{i}^{2}$
вычисляется по конечному числу измерений, а потому даёт лишь приближённое
значение (\emph{оценку}) для величины дисперсии. Кроме того, при этом
мы вынуждены использовать \emph{выборочное} среднее $\overline{x}=\frac{1}{n}\sum x_{i}$,
что вносит дополнительную ошибку в вычисление $\sigma$. Оказывается,
что при малых $n$ эта ошибка может давать сильно заниженные результаты.
Математическая статистика рекомендует использовать слегка модифицированную
формулу --- так называемую называемой \emph{несмещённую}
оценку среднеквадратичного отклонения:
\begin{equation}
\boxed{s_{x}^{2}=\frac{1}{n-1}\sum\limits _{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}},\label{eq:sigma_straight}
\end{equation}
то есть сумму квадратов отклонений нужно делить не на полное число
слагаемых $n$, а уменьшенное на единицу.

Погрешность, вычисленную по формуле (\ref{eq:sigma_straight}), называют
также \emph{стандартным отклонением} от среднего. Эту формулу имеет
смысл применять при $n<10$. При $n\ge10$ результаты вычислений по
формулам (\ref{eq:sigma}) и (\ref{eq:sigma_straight}) отличаются
уже не более, чем на 5\%.

Коэффициент $n-1$ можно интерпретировать следующим образом. Отклонения
от выборочного среднего $\Delta x_{i}=x_{i}-\overline{x}$ подчиняются
очевидному соотношению $\sum\limits _{i=1}^{n}\Delta x_{i}=0$, поэтому
лишь $n-1$ из них являются независимыми. Таким образом, величину
$n-1$ можно назвать \emph{числом степеней свободы} для отклонений
измеряемой величины.

\example{
    Общее доказательство
    того, что оценка (\ref{eq:sigma_straight}) <<лучше>>,
    чем (\ref{eq:sigma}) (т.\,е. является <<несмещенной>>),
    довольно громоздко. В качестве относительно простого примера рассмотрим
    случай $n=2$. Пусть физическая величина $x$ имеет нормальное распределение
    с нулевым средним $\bar{x}=0$ (это не ограничивает общности, поскольку
    всегда можно сделать замену переменных $x-x_{0}\to x$) и дисперсией
    $s^{2}$. По выборке из двух измерений $\left\{ x_{1},\thinspace x_{2}\right\} $
    находим оценки для среднего
    \[
    \overline{x}=\frac{x_{1}+x_{2}}{2},
    \]
    (заметим, что вообще говоря $\left\langle x\right\rangle \ne0$) и
    среднеквадратичного отклонения 
    \[
    s_{x}^{2}=\frac{1}{2}\left[\left(x_{1}-\frac{x_{1}+x_{2}}{2}\right)^{2}+\left(x_{2}-\frac{x_{1}+x_{2}}{2}\right)^{2}\right]=\frac{1}{4}x_{1}^{2}-\frac{1}{2}x_{1}x_{2}+\frac{1}{4}x_{2}^{2}.
    \]
    
    Проведём усреднение полученных выражений по большому
    числу опытов, в каждом из которых проводится по $n=2$ измерений.
    Обозначим такое усреднение угловыми скобками. Тогда
    \[
    \left\langle \overline{x}\right\rangle =\frac{\left\langle x_{1}\right\rangle +\left\langle x_{2}\right\rangle }{2}=\left\langle x\right\rangle =0.
    \]
    Ввиду независимости измерений имеем $\left\langle x_{1}\cdot x_{2}\right\rangle =0$,
    так что
    \[
    \left\langle s_{x}^{2}\right\rangle =\frac{1}{4}\left\langle x_{1}^{2}\right\rangle +\frac{1}{4}\left\langle x_{2}^{2}\right\rangle =\frac{1}{2}\sigma^{2}\ne\sigma^{2}.
    \]
    Таким образом, оценка для среднего стремится к правильному пределу
    $\left\langle x\right\rangle =0$, однако оценка дисперсии по формуле
    (\ref{eq:sigma}) стремится к <<смещённому>> значению $\frac{1}{2}\sigma^{2}$, вдвое отличающемуся от правильного.
    Видно, что если бы мы воспользовались формулой (\ref{eq:sigma_straight}),
    то результат получился бы 
    \emph{несмещенный}.
    
    Чтобы разобраться в причине <<смещения>>,
    посмотрим, что получится, если вместо выборочного среднего $\overline{x}$
    использовать предельное $\left\langle x\right\rangle =x_{0}=0$:
    \[
    s_{x}^{2}=\frac{1}{2}\left(x_{1}^{2}+x_{2}^{2}\right)\qquad\to\qquad\left\langle s_{x}^{2}\right\rangle =\frac{1}{2}\left\langle x_{1}^{2}\right\rangle +\frac{1}{2}\left\langle x_{2}^{2}\right\rangle =\sigma^{2}.
    \]
    Видно, что оценка получается <<правильной>>,
    т.е. несмещенной. Таким образом, ошибка в оценке $\sigma$ возникает
    из-за использования выборочного среднего $\overline{x}$ вместо <<истинного>>
    $\left\langle x\right\rangle =\lim\limits_{n\to\infty}\overline{x}$.
}

\subsection{Эффективность}

Для сравнения разных методов оценки, очень важным свойством является
эффективность. Говоря простым языком, эффективность - это величина,
обратная разбросу значений $\hat{\theta}$ при применении к разным
наборам данных. Для того, чтобы хорошо разобраться в этом свойстве, надо
вспомнить, что оценка, как случайная величина, распределена с плотностью
$P(\hat\theta | \theta)$. Вид этого распределения может быть не
известен полностью, но знать его свойства -- низшие моменты --
необходимо. Среднее по нему суть смещение, а дисперсия
$\sigma_{\hat\theta}^2 = \int{ (\hat\theta - \theta} ) P(\hat\theta | \theta) d\hat\theta$
суть мера ошибки в определении оценки. Выбирая между различными
методами, мы, естественно, хотим, чтобы ошибка параметра была
минимальной из всех доступных нам способов его определения для
фиксированного эксперимента. Разные методы обладают разной
эффективностью и в общем случае при конечной статистике дисперсия
распределения оценки никогда не будет равна нулю. Разумеется, встает
вопрос о том, можно ли построить оценку с максимальной возможной
эффективностью.

\subsection{Граница Рао-Крамера и информация}
\disclaimer{Материал этого раздела предназначен для продвинутого изучения статистики}

\paragraph{Утверждение}

Пусть есть несмещенная оценка $\hat\theta$ параметра $\theta$, тогда
всегда выполняется неравенство:

\begin{equation}
  D(\hat\theta) \geq \frac{1}{I(\theta)},
\end{equation} где $I(\theta)$ - информация Фишера:

\begin{equation}
  I(\theta) = E_X \left[\left( \frac{\partial \ln L(X,\theta)}{\partial \theta} \right)^2 \right],
\end{equation} где в свою очередь $L(X, \theta)$ - функция
правдоподобия. $L(X,\theta) = \prod{P(X_i,\theta)}$ - вероятность
получить набор данных при фиксированном значении $\theta$.

\paragraph{Доказательство в одномерном случае}

Введем функцию

\begin{equation}
  W = \frac{\partial \ln L(X,\theta)} {\partial \theta}.
\end{equation}

Найдем математическое ожидание этой функции:

\begin{equation}
  E(W) = \int{L(X, \theta) W(X, \theta) dX} = \int{ L \frac{1}{L} \frac{\partial L}{\partial \theta} dX} = \frac{\partial}{\partial \theta} \int{L dX} = 0.
\end{equation}

Теперь рассмотрим ковариацию $E(\hat\theta W)$:

\begin{equation}
  E(\hat\theta W) = \int{\hat\theta \frac{1}{L} \frac{\partial L} {\partial {\theta}} L dX} = \frac{\partial}{\partial \theta} \int{\hat\theta L dX} =  \frac{\partial}{\partial \theta} E(\hat\theta).
\end{equation}

Для несмещенных оценок последнее выражение упрощается до вида
$E(\hat\theta W) = {\partial \theta}/{\partial \theta} = 1$ Согласно
неравенствую
\href{https://ru.wikipedia.org/wiki/\%D0\%9D\%D0\%B5\%D1\%80\%D0\%B0\%D0\%B2\%D0\%B5\%D0\%BD\%D1\%81\%D1\%82\%D0\%B2\%D0\%BE_\%D0\%9A\%D0\%BE\%D1\%88\%D0\%B8_\%E2\%80\%94_\%D0\%91\%D1\%83\%D0\%BD\%D1\%8F\%D0\%BA\%D0\%BE\%D0\%B2\%D1\%81\%D0\%BA\%D0\%BE\%D0\%B3\%D0\%BE}{Коши
--- Буняковского}, получаем:
$\sqrt{D(\hat\theta) D(W)} \geq \left| E(\hat\theta W) \right| = 1$.
Отсюда легко получаем желаемое утверждение.

\paragraph{Следствие}

Максимальная эффективность достигается в том случае, если величины
$\hat\theta$ и $W$ являются скоррелированными. Оценка,
максимизирующая функцию $L(X,\theta)$ является в общем случае
состоятельной, несмещенной, кроме того совпадает с оценкой вида
$W(\hat\theta, X) = 0$. То есть является максимально эффективной.

\section{Интервальные оценки}

    На практике применение точечных оценок сильно затруднено тем, что не
известно, на сколько каждая такая оценка точна. Действительно, мы можем
спокойно утверждать, что слон весит один килограмм если разброс нашей
оценки составляет больше массы слона. Для того, чтобы решить эту
проблему есть два пути. Первый путь - это на ряду с точечной оценки
указывать меру эффективности этой оценки или ее
разброс$\sigma_{\hat\theta}$. Но тут любой внимательный слушатель
заметит, что для определения эффективности, вообще говоря, надо знать
истинное значение пареметра $\theta$, которого мы, разумеется не
знаем. Следовательно приходится использовать не эффективность, а оценку
этой эффективности, которая сама по себе является случайной величиной.
Кроме того, часто случается, что распределение оценки является не
симметричным и описать его одним числом не удается.

Более корректным способом является построение интервальной оценки
(доверительного интервала). Формально определение интервальной оценки
будет отличаться в зависимости от того, какое определение вероятности вы
будете использовать.

\textbf{Частотная интерпретация}: интервальной оценкой параметра или
группы параметров $\theta$ с уровнем достоверности $\alpha$
называется такая область на пространстве параметров (в одномерном случае
- интервал), которая при многократном повторении эксперимента с
вероятностью (частотой) $\alpha$ перекрывает истинное значение
$\theta$.

\textbf{Субъективная интерпретация}: доверительным интервалом для
параметров $\theta$ будем называть такую область в пространстве
параметров, в которой интегральная апостериорная вероятность нахождения
истинного значения параметра равна $\alpha$.

Для точного описания результата проведения анализа как правило в
качестве результата приводят как точечную оценку, так и интервальную
оценку с некоторым уровнем достоверности (в английском варианте
Confidence Level или C. L.). В некоторых случаях приводят несколько
интервальных оценок с разным уровнем достоверности. В случае, когда речь
идет об определении верхней или нижней границы какого-то параметра,
точечная оценка как правило не имеет смысла и в качестве результата
дается только итнервальная оценка.

\note{ 
Точечная оценка также не имеет смысла в случае, когда
распределение оценки, скажем имеет вид однородного распределения на
отрезке. В этом случае все параметры на этом отрезке равновероятны и не
понятно, какой из них называть результатом.
}

\note{
Вполне очевидно, что для одних и тех же данных с
использованием одного и того же метода оценивания можно построить
бесконечное множество интервальных оценок с фиксированным уровнем
достоверности. Действительно, мы можем двигать интервал в разные стороны
таким образом, чтобы его вероятностное содержание не менялось. Обычно,
если не оговорено иначе, используются так называемые центральные
доверительные интервалы, в которых вероятностные содержание за границами
интервалов с обеих сторон равны.
}

\section{Методы построения оценок}

\subsection{Метод максимума правдоподобия}

\paragraph{Определение}

Введем функцию правдоподобия следующим образом:
$L(X,\theta) = \prod{P(X_i,\theta)}$, где $P(X_i, \theta)$ -
вероятность получить случайным образом компоненту данных с номером $i$
при истинном значении параметра $\theta$. Будем называеть оценкой
максимума правдоподобия такую $\hat\theta$, для которой
$L(\hat\theta, X)$ максимально для заданного набора данных. Такая
оценка в общем случае является состоятельной и несмещенной. Она имеет
очень хорошо понятный физический смысл: $\hat\theta$ - это такое
значение параметра, для которого вероятность получить набор данных
максимальна. Кроме того, как упоминалось в параграфе \textbf{3.1.2},
такая оценка кроме всего прочего будет еще и наиболее эффективной из
всех возможных (достаточной статистикой). 

\paragraph{Ограничения Оценка}
является эффективной только если область измереия $X$ не зависит от
$\theta$. 

\paragraph{Интервальная оценка} Для построения интервальной
оценки воспользуемся субъективной вероятностью и теоремой Байеса:
\begin{equation}
  P(\theta | X) = \frac{L(X|\theta) \pi(\theta)}{\int{LdX}}
\end{equation} Здесь $L$ - функция правдоподобия, а $\pi$ -
априорная вероятность для параметра $\theta$. Если нет никакой
дополнительной информации оп параметре, то мы можем положить
$\pi = 1$. Мы получаем, что распределение значения реального параметра
повторяет форму функции правдоподобия. Вероятность того, что параметр
$\theta$ лежит в диапазоне от $a$ до $b$ составляет
\begin{equation}
  P(a < \theta < b) = \int_b^a{L(X | \theta)}.
\end{equation} 

Интервальная оценка в асимптотическом случае
Согласно центральной предельной теореме, при достаточно большом
количестве данных $X$ (и при разумных предположениях о форме
распределения для этих данных), функция правдоподобия будет иметь вид
нормального распределения. В этом случае центральный доверительный
интервал можно вычислить, пользуясь аналитической формулой. Центральный
доверительный интервал с уровнем значимости 68\% будет соответствовать
диапазону между значениями $\theta$, такими, что значение функции
правдоподобия в них отличается от максимума на 0.5.

\paragraph{Логарифмическое правдоподобие}
В реальных задачах часто работают не с
самой функцией правдоподобия, а с ее логарифмом
$\ln L(X|\theta) = \sum{\ln P(X_i | \theta)}$. Это связано с тем, что
при численных манипуляциях проще работать со сложением, чем с
умножением. Очевидно, что максимум правдоподобия будет одновременно и
максимумом его логарифма и обратно. Для построения интервальных оценок
логарифм прадоподобия не годится.


\subsection{Метод минимума \texorpdfstring{$\chi^{2}$}{chi2}}
\label{sec:chi2}

    Разберем частный, но очень часто встречающийся случай, когда
распределение измеренной величины - нормальное (Гауссово):
\begin{equation}
  P(X_i | \theta) = \frac{1}{\sqrt {2 \pi} \sigma_i} e^{- \frac{(X_i - \mu_i(\theta))^2}{2 \sigma_i^2}}
\end{equation} Здесь $\mu_i(\theta)$ - модельное значение. Опуская
нормировочный множитель, не существенный для нахождения максимума можно
записать логарифм функции правдоподобия в виде: \begin{equation}
  \ln L(X_i | \theta) = - \sum{\frac{(X_i - \mu_i(\theta))^2}{2 \sigma_i^2}}
\end{equation} Определим величину \begin{equation}
    \chi^2 = - 2 \ln L(X_i | \theta) =  \sum{\frac{(X_i - \mu_i(\theta))^2}{\sigma_i^2}}.
\end{equation} Назовем оценкой минимума $\chi^2$ такое значение
параметра, при котором эта величина минимально. Очевидно, что эта оценка
эквивалента максимуму правдоподобия для нормально расспределенных
измерений. Оценка минимума $\chi^2$ будет состоятельной и несмещенной
и для других распределений измерений, но только для таких измерений она
будет обладать максимальной эффективностью.

\subsubsection{Интервальная оценка}

Также как и в методе максимума правдоподобия, центральный интервал можно
получить из аналитической форму функции правдоподобия (для нормально
распределенных ошибок, форма правдоподобия будет нормальной всегда, а не
только в асимптотике). Из-за множителя 2, мы получаем, что центральный
интервал в 68\% будет соответствовать отклонению $\chi^2$ на 1.
Отклонение на 2 будет соответствовать уже 95\% доверительному интервалу.

\subsubsection{Проверка качества фита}

Дополнительное полезоное свойство суммы $\chi^2$ заключается в том,
что она позволяет оценить, насколько хорошо данные описываются моделью.
В том случае, когда измерения распределены по нормальному закону и
независимы между собой, сумма $\chi^2$ оказывается распределена по
\href{https://ru.wikipedia.org/wiki/\%D0\%A0\%D0\%B0\%D1\%81\%D0\%BF\%D1\%80\%D0\%B5\%D0\%B4\%D0\%B5\%D0\%BB\%D0\%B5\%D0\%BD\%D0\%B8\%D0\%B5_\%D1\%85\%D0\%B8-\%D0\%BA\%D0\%B2\%D0\%B0\%D0\%B4\%D1\%80\%D0\%B0\%D1\%82}{одноименному
распределению}. При хорошем соответствии модели и данных величина
$\chi\^{}2 / n $, где $n$ - так называемое количество степеней
свободы (количество точек минус количество параметров), должна в среднем
быть равна 1. Значения существенно большие (2 и выше) свидетельствуют о
плохом соответствии или заниженных погрешностях. Значения меньше 0.5 как
правило свидетельствуют о завышенных ошибках.


\subsection{Метод наименьших квадратов}

    В случа, если все ошибки $\sigma_i$ одинаковы, множитель
$\frac{1}{\sigma^2}$ можно вынести за скобку. Для нахождения минимума
постоянный множитель не важен, поэтому мы можем назвать оценкой
наименьших кавдратов такое значение параметра $\hat\theta$, при
котором миниальна сумма квадратов: \begin{equation}
  Q = \sum{(X_i - \mu_i(\theta))^2}.
\end{equation} Эта оценка очевидно сохраняет все свойства оценки
минимума $\chi^2$, правда только в том случае, если все ошибки
действительно одинаковы.

Оценка наименьших квадратов удобна в том случае, когда не известны
ошибки отдельных измерений. Она является состоятельной и асимптотически
несмещенной, хотя ее эффективность оптимальна только для ограниченного
количества случаев. В некоторых случаях, ошибку измерения можно оценить
по разбросу данных, используя критерий Пирсона.

\paragraph{Интервальная оценка}

Оценка методом наименьших квадратов очевидно игнорирует информацию о
погрешностях измерений и не позволяет напрямую оценить погрешность
результата без дополнительных предположений. Для получения оценки
погрешностей надо сделать три предположения:

\begin{itemize}
    \item  Данные описываются предложенной моделью;
    \item  Отклонения данных от модельной кривой независимы между собой (носят
  статистический характер);
    \item  Статистические ошибки для всех точек равны между собой.
\end{itemize}

При этих предположениях, можно оценить $\sigma$ для каждой из точек
как средне квадратичное отклонение точек от наилучшей модели (той,
которая получена минимизацией суммы $Q$). После этого задача получения
погрешностей сводится к уже решенной для метода $\chi^2$.

\note{
По очевидным причинам оценка погрешностей, проведенная таким образом, не имеет смысла для маленького количества измерений (меньше 8-10 точек). Все будет работать и будет получен какой-то результат, но он будет довольно бессмысленным.
}


\note{
Одна из основных проблем, связанных с определением погрешностей методом наименьших квадратов заключается в том, что он дает разумные погрешности даже в том случае, когда данные вообще не соответствуют модели. По этой причине не рекомендуется использовать его в тех случаях, когда погрешности измеренных значений известны. Если других инструментов под рукой нет, то результаты работы метода надо всегда проверять визуально по графику.
}

\subsection{Другие методы}

    Существует довольно большое количество других методов определения
параметров зависимости. Как правило они специально заточены под
конкретную задачу. В качестве примера можно привести
\href{https://ru.wikipedia.org/wiki/\%D0\%9C\%D0\%B5\%D1\%82\%D0\%BE\%D0\%B4_\%D0\%BC\%D0\%BE\%D0\%BC\%D0\%B5\%D0\%BD\%D1\%82\%D0\%BE\%D0\%B2}{метод
моментов}. Методо дает состоятельную и несмещенную, но довольно слабо
эффективную оценку, но при этом позволяет очень быстро делать оценки для
больших объемов данных. Еще один пример -
\href{https://arxiv.org/abs/physics/0604127}{метод квази-оптимальных
весов Ф. В. Ткачева}, напротив обладает высокой эффективностью и
стабильностью, но несколько сложнее в реализации.

\section{Многопараметрические оценки}

    Однопараметрические оценки очень просты для понимания и реализации, но
довольно редко встречаются на практике. Даже при оценке параметров
линейно зависимости вида $y = k x + b$ уже существует два параметра:
$k$ - наклон прямой и $b$ - смещение. Все перечисленные выше
математические методы отлично работают и в многомерном случае, но
процесс поиска экстремума функции (максимума в случае метода максимума
правдоподобия и минимума в случае методов семейства наименьших
квадратов) и интерпретация результатов требуют использования специальных
программных пакетов.

    
\subsection{Доверительные области в многомерном случае}

    Принцип построения доверительной области в многомерном случае точно
такой же, как и для одномерных доверительных интервалов. Требуется найти
такую областью пространства параметров $\Omega$, для которой
вероятностное содержание для оценки параметра $\hat \theta$ (или
самого параметра $\theta$ в засимости от того, какой философии вы
придерживаетесь) будет равно некоторой наперед заданной величине
$\alpha$: \begin{equation}
  P(\theta \in \Omega) = \int_\Omega{L(X | \theta)}d\Omega = \alpha.
\end{equation}

Реализация на практике этого определения сталкивается с тремя
проблемами:

\begin{enumerate}
\item Взятие многомерного интеграла от произвольной функции - не тривиальная
  задача. Даже в случае двух параметров, уже требуется некоторый уровень
  владения вычислительной математикой и компьютерными методами. В случае
  большего числа параметров, как правило надо использовать специально
  разработанные для этого пакеты.
\item Определить центральный интервал для гипер-области гораздо сложнее, чем
  сделать это для одномерного отрезка. Единых правил для выбора такой
  области не существует.
\item Даже если удалось получить доверительную область, описать такой объект
  в общем случае не так просто, так что представление результатов
  составляет определенную сложность.
\end{enumerate}

Для решения этих проблем, пользуются следующим приемом: согласно
центральной предельной теореме, усреднение большого количества одинаково
распределенных величин дает нормально распределенную величину. Это же
верно и в многомерном случае. В большинстве случаев, мы ожидаем, что
функция правдоподобия будет похожа на многомерное нормальное
распределение: \begin{equation}
    L(\theta) = \frac{1}{(2 \pi)^{n/2}\left|\Sigma\right|^{1/2}} e^{-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)}
\end{equation} где n - размерность вектора параметров, $\mu$ - вектор
наиболее вероятных значений, а $\Sigma$ -
\href{https://ru.wikipedia.org/wiki/\%D0\%9A\%D0\%BE\%D0\%B2\%D0\%B0\%D1\%80\%D0\%B8\%D0\%B0\%D1\%86\%D0\%B8\%D0\%BE\%D0\%BD\%D0\%BD\%D0\%B0\%D1\%8F_\%D0\%BC\%D0\%B0\%D1\%82\%D1\%80\%D0\%B8\%D1\%86\%D0\%B0}{ковариационная
матрица} распределения.

Для многомерного нормального распределения, линии постоянного уровня (то
есть поверхности, на которых значение плотности вероятности одинаковые)
имеют вид гипер-эллипса, определяемого уравнением
$(x - \mu)^T \Sigma^{-1} (x - \mu) = const$. Для любого вероятностного
содержания $\alpha$ можно подобрать эллипс, который будет
удовлетворять условию на вероятностное содержание. Интерес правда
редставляет не эллипс (в случае размерности больше двух, его просто
невозможно отобразить), а ковариацонная матрица. Диагональные элементы
этой матрицы являются дисперсиями соответствующих параметров (с учетом
всех корреляций параметров).

    
\section{Аналитическая оценка для линейной модели}

Рассмотрим математически более строгий метод построения наилучшей
прямой $y=kx+b$ по набору экспериментальных точек 
\[
\left\{ \left(x_{i},y_{i}\right),i=1\ldots n\right\} .
\]

Расстояние от экспериментальной точки от искомой прямой, измеренное
по вертикали, равно
\[
\Delta y_{i}=y_{i}-\left(kx_{i}+b\right).
\]
Найдём такие коэффициенты $k$ и $b$, чтобы сумма квадратов таких
расстояний для всех точек была минимальной:
\begin{equation}
S\!\left(k,b\right)=\sum\limits _{i=1}^{n}\Delta y_{i}^{2}\to\mathrm{min}.\label{eq:mnk_S}
\end{equation}
Данный метод построения наилучшей прямой называют \emph{методом наименьших
квадратов} (МНК).

Рассмотрим сперва более простой частный случай. Пусть заведомо известно,
что искомая прямая проходит через ноль, то есть $b=0$ и $y=kx$.
Необходимое условие минимума функции $S\left(k\right)$, как известно,
есть равенство нулю её производной. Дифференцируя сумму (\ref{eq:mnk_S})
по $k$, считая все величины $\left\{ x_{i},\,y_{i}\right\} $ константами,
найдём 
\[
\frac{dS}{dk}=-\sum\limits _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}\right)=0.
\]
Решая относительно $k$, находим 
\[
k=\frac{\sum\limits _{i=1}^{n}x_{i}y_{i}}{\sum\limits _{i=1}^{n}x_{i}^{2}}.
\]
Поделив числитель и знаменатель на $n$, этот результат можно записать
более компактно:
\begin{equation}
\boxed{k=\frac{\left\langle xy\right\rangle }{\left\langle x^{2}\right\rangle }}.\label{eq:MNK0}
\end{equation}
Угловые скобки означают усреднение по всем экспериментальным точкам:
\[
\left\langle \ldots\right\rangle \equiv\frac{1}{n}\sum\limits _{i=1}^{n}\left(\ldots\right)_{i}
\]

В общем случае при $b\ne0$ функция $S\left(k,b\right)$ должна иметь
минимум как по $k$, так и по $b$. Поэтому имеем систему из двух
уравнений:
\begin{align*}
\frac{\partial S}{\partial k} & =-\sum\limits _{i=1}^{n}2x_{i}\left(y_{i}-kx_{i}-b\right)=0,\\
\frac{\partial S}{\partial b} & =-\sum\limits _{i=1}^{n}2\left(y_{i}-kx_{i}-b\right)=0.
\end{align*}
Решая систему, можно получить
\begin{equation}
\boxed{k=\frac{\left\langle xy\right\rangle -\left\langle x\right\rangle \left\langle y\right\rangle }{\left\langle x^{2}\right\rangle -\left\langle x\right\rangle ^{2}},\qquad b=\left\langle y\right\rangle -k\left\langle x\right\rangle }.\label{eq:MNK}
\end{equation}
Эти соотношения и есть решение задачи о построении наилучшей прямой
методом наименьших квадратов.

{\footnotesize{}Совсем кратко формулу (\ref{eq:MNK}) можно записать,
если ввести обозначение
\begin{equation}
D_{xy}\equiv\left\langle xy\right\rangle -\left\langle x\right\rangle \left\langle y\right\rangle =\left\langle \left(x-\left\langle x\right\rangle \right)\cdot\left(y-\left\langle y\right\rangle \right)\right\rangle \label{eq:cov}
\end{equation}
(в математической статистике $D_{xy}$ называют }\emph{\footnotesize{}ковариацией}{\footnotesize{};
при $x\equiv y$ имеем дисперсию $D_{xx}=\left\langle \left(x-\left\langle x\right\rangle \right)^{2}\right\rangle $).
Тогда
\begin{equation}
k=\frac{D_{xy}}{D_{xx}},\qquad b=\left\langle y\right\rangle -k\left\langle x\right\rangle .\label{eq:MNK_short}
\end{equation}
}{\footnotesize\par}

\paragraph{Погрешность МНК.}

Найдём погрешности $\sigma_{k}$ и $\sigma_{b}$ коэффициентов, вычисленных
по формуле (\ref{eq:MNK}) (или (\ref{eq:MNK0})).

Сделаем следующие предположения: погрешность измерений величины $x$
пренебрежимо мала: $\sigma_{x}\approx0$, а погрешность по $y$ одинакова
для всех экспериментальных точек $\sigma_{y}=\mathrm{const}$ и имеет
случайный характер (систематическая погрешность отсутствует).

Пользуясь в этих предположениях формулами для погрешностей косвенных
измерений (см. раздел (\ref{sec:kosv})) можно получить следующие
соотношения (выкладки здесь весьма громоздки, подробности можно найти
в п. (\ref{subsec:MMP})):
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-2}\left(\frac{D_{yy}}{D_{xx}}-k^{2}\right)},\label{eq:MNK_sigma}
\end{equation}
\begin{equation}
\sigma_{b}=\sigma_{k}\sqrt{\left\langle x^{2}\right\rangle },\label{eq:MNK_sigma_b}
\end{equation}
где использованы введённые выше сокращённые обозначения (\ref{eq:cov}).
Коэффициент $n-2$ отражает число независимых <<степеней
свободы>>: $n$ экспериментальных точек за вычетом двух
условий связи (\ref{eq:MNK}).

В частном случае $y=kx$ имеем
\begin{equation}
\sigma_{k}=\sqrt{\frac{1}{n-1}\left(\frac{\left\langle y^{2}\right\rangle }{\left\langle x^{2}\right\rangle }-k^{2}\right)}.\label{eq:MNK_sigma0}
\end{equation}


\paragraph{Условия применимости МНК.}

Формулы (\ref{eq:MNK}) (или (\ref{eq:MNK0})) позволяют провести
прямую по \emph{любому} набору экспериментальных данных, а формулы
(\ref{eq:MNK_sigma}) (или (\ref{eq:MNK_sigma0})) --- вычислить
соответствующую среднеквадратичную ошибку для её коэффициентов. Однако
далеко не всегда результат будет иметь физический смысл. Перечислим
ограничения применимости данного метода.

В первую очередь метод наименьших квадратов --- статистический,
и поэтому он предполагает использование достаточно большого количества
экспериментальных точек (желательно $n>10$).

Поскольку метод предполагает наличие погрешностей только по $y$,
оси следует выбирать так, чтобы погрешность $\sigma_{x}$ откладываемой
по оси абсцисс величины была минимальна.

Кроме того, метод предполагает, что все погрешности в опыте ---
случайны. Соответственно, формулы (\ref{eq:MNK_sigma}) и (\ref{eq:MNK_sigma0})
применимы \emph{только для оценки случайной составляющей} ошибки $k$
и $b$. Если в опыте предполагаются достаточно большие систематические
ошибки, они должны быть оценены \emph{отдельно}. Отметим, что для
оценки систематических ошибок не существует строгих математических
методов, поэтому в таком случае проще и разумнее всего воспользоваться
описанным выше графическим методом.

Наконец, стоит предостеречь от использования МНК <<вслепую>>,
без построения графика. Этот метод неспособен выявить такие <<аномалии>>,
как отклонения от линейной зависимости, немонотонность, случайные
всплески и т.п. Все эти случаи могут быть легко обнаружены при построении
графика и требуют особого рассмотрения.

Резюмируя, можно сформулировать универсальную практическую рекомендацию:
если результаты какого-либо математического метода обработки данных
существенно расходятся с тем, что можно получить <<вручную>>
графически, есть все основания сомневаться в применимости метода в
данной ситуации.